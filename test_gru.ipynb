{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_gru.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMU/+c0RfFajDGo7Qk1YFWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithinps021/Project_IISU/blob/main/test_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PJvuIFyD1xU7"
      },
      "outputs": [],
      "source": [
        "from urllib import response\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Bidirectional, Dropout, Dense, Activation, Flatten, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential,Model,load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls=open('classes','rb')\n",
        "res=open('responses','rb')\n",
        "tok=open('tokenizer','rb')\n",
        "\n",
        "classes=pickle.load(cls)\n",
        "responses=pickle.load(res)\n",
        "Tokenizer=pickle.load(tok)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8YD9MrjO156T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp =spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def normalize(text):\n",
        "  text=nlp(text)\n",
        "  norm_text=[]\n",
        "  for token in text:\n",
        "    if not token.is_punct and not token.is_stop and not token.is_space:\n",
        "      norm_text.append(token.lemma_.lower())\n",
        "  return ' '.join(norm_text)\n"
      ],
      "metadata": {
        "id": "jzcfB0KD4cHs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenizer_vocab_size = len(Tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(Tokenizer_vocab_size,100, input_length = 100)) # Accepts the vector inputs\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))) \n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(800, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(classes), activation='softmax'))\n",
        "\n",
        "model.load_weights('gru3.h5')\n",
        "\n",
        "print('Chatbot started. type /stop to stop the bot')\n",
        "\n",
        "one_word=[\"hello\",\"hi\",\"thanks\",\"ok\",\"hey\"]\n",
        "\n",
        "def runChatbot():\n",
        "    while(True):\n",
        "        sen = input('QUERY : ')\n",
        "        if sen == '/stop':\n",
        "            break\n",
        "        if sen not in one_word and len(sen.split(' '))==1:\n",
        "            print(\"RESPONSE : sorry i didnt get you\")\n",
        "            continue\n",
        "        sen = normalize(sen)\n",
        "        tokens = Tokenizer.texts_to_sequences([sen])\n",
        "        tokens = pad_sequences(tokens, maxlen=100)\n",
        "        prediction = model.predict(np.array(tokens))\n",
        "        pred = np.argmax(prediction)\n",
        "        print(pred, np.max(prediction))\n",
        "        print('RESPONSE : ', responses[classes[pred]])\n",
        "\n",
        "runChatbot()"
      ],
      "metadata": {
        "id": "0GrXEb7T4iOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}